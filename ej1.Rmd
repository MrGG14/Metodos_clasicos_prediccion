---
title: "Práctica Métodos Clásicos Para La Predicción"
author: "Javier Gil Domínguez, David Lazaro Martin, Nicolas Vega Muñoz"
date: "2022-11-25"
output: pdf_document
---

**EJERCICIO 1**

El Instituto Nacional de la Diabetes y las Enfermedades Digestivas y Renales (EE.UU.) realizó un 
estudio sobre 768 mujeres indias Pima adultas que vivían cerca de Phoenix. Se registraron las 
siguientes variables: 
• número de embarazos, 
• concentración de glucosa en plasma a las 2 horas en una prueba de tolerancia a la glucosa 
oral, 
• presión arterial diastólica (mm Hg), 
• grosor del pliegue cutáneo del tríceps (mm), 
• insulina sérica a las 2 horas (mu U/ml), 
• índice de masa corporal (peso en kg/(altura en m2)), 
• función de pedigrí de la diabetes, 
• edad (años) y 
• una prueba de si la paciente muestra signos de diabetes (codificada como 0 si es negativa, 
1 si es positiva). 
Los datos pueden obtenerse en el repositorio de bases de datos de aprendizaje automático de 
la UCI en http://www.ics.uci.edu/˜mlearn/MLRepository.html. El correspondiente dataset está 
disponible en el conjunto de datos pima del paquete faraway.
Se solicita utilizar la información anterior para construir un modelo de regresión lineal que 
permita predecir la concentración de glucosa en plasma a las 2 horas en una prueba de 
tolerancia a la glucosa oral. En concreto, se solicita:

##a) Importar y preparar las variables (variables categóricas como factors y poner etiquetas para los posibles valores de las variables categóricas)##

```{r}
library(faraway)
data(pima)
pima$test <- factor(pima$test, labels=c("negative", "positive"))
```

##b) Hacer un estudio de estadística descriptiva sobre las variables disponibles, incluyendo el análisis de valores ausentes y atípicos##

Primero comprobamos la existencia de valores ausentes:

```{r}
sum(is.na(pima)==TRUE)
summary(pima)
```

Vemos que nuestro dataset está formado por 9 variables de las cuales test es categórica y está compuesta por 500 valores negative y 268 positive. 
También vemos que tenemos valores iguales a 0 en todas las variables excepto en diabetes y age. Consultando en internet, consideramos que estos valores (menos en la columna pregnant) se deben a un error ya que no es posible tener unos niveles tan bajos de, por ejemplo, glucosa. Por lo tanto, vamos a sustituirlos por NA's (valores ausentes en R). Posteriormente decidiremos como tratar dichos valores ausentes.

```{r}
pima$diastolic[pima$diastolic == 0] <- NA
pima$glucose[pima$glucose == 0] <- NA
pima$triceps[pima$triceps == 0] <- NA
pima$insulin[pima$insulin == 0] <- NA
pima$bmi[pima$bmi == 0] <- NA

print(paste("Valores ausentes en diastolic:",sum(is.na(pima$diastolic)==TRUE)))
print(paste("Valores ausentes en glucose:",sum(is.na(pima$glucose)==TRUE)))
print(paste("Valores ausentes en triceps:",sum(is.na(pima$triceps)==TRUE)))
print(paste("Valores ausentes en insulin:",sum(is.na(pima$insulin)==TRUE)))
print(paste("Valores ausentes en bmi:",sum(is.na(pima$bmi)==TRUE)))
print(paste("Numero total de Valores ausentes:",sum(is.na(pima)==TRUE)))
which_nas <- apply(pima, 1, function(X) any(is.na(X)))
#eliminamos las filas que esten completamente vacias en caso de que haya
pima = pima[rowSums(is.na(pima)) != ncol(pima),]
print(paste("Numero de filas con algun valor ausente:",length(which(which_nas))))


```
Observamos que hay una gran cantidad de valores ausentes (652), sobre todo en las variables triceps e insulina. Puesto que estos aparecen repartidos en 376 filas no consideramos oportuno eliminarlas, por lo que procedemos a imputar los valores mediante KNN, estableciendo el numero de vecinos a 10. 

```{r}
library(multiUS)
pima=KNNimp(pima, k = 10, scale = TRUE)
```
#GRÁFICOS UNIDIMENSIONALES

Con estos gráficos podemos hacernos una idea de la distribución de nuestros datos de algunas columnas. 

Analizamos la glucosa, que es la variable que tenemos que predecir. Observamos que tiene una distribución similar a una normal de media 121.7 y no hay valores atípicos.

```{r}
summary(pima$glucose)
hist(pima$glucose, ylab = "", col = "grey", main = "", breaks=20)
lines(density(pima$glucose, na.rm = TRUE))
par(new = TRUE) # Esto indica que lo dibujaremos sobre el gráfico anterior
boxplot(pima$glucose, horizontal = TRUE, axes = FALSE, lwd = 2, col = rgb(0, 1, 1, alpha = 0.15))
```

Para el número de embarazos observamos que la mayoria tiene entre 0 y 3 hijos y que existen algunos valores atípicos. Estos datos, a pesar de ser atípicos, no los eliminaremos puesto que no podemos consultar a un experto, son escasos, y tampoco podemos asegurar que sean imposibles.

```{r}
summary(pima$pregnant)
hist(pima$pregnant, ylab = "", col = "grey", main = "", breaks=20)
lines(density(pima$pregnant)) # lines indica que se va a dibujar una línea sobre el gráfico anterior
par(new = TRUE) # Esto indica que lo dibujaremos sobre el gráfico anterior
boxplot(pima$pregnant, horizontal = TRUE, axes = FALSE, lwd = 2, col = rgb(0, 1, 1, alpha = 0.15))
```

A continuación vamos a analizar la presión arterial diastólica. Tiene una distribución similar a una normal con media entorno a 72, coincidente con la mediana. Vemos que la distribucion concuerda con los valores normales de la presión diastólica (entre 60 y 80) puesto que el primer cuartil es 64 y el tercero 80 exactamente, el 50% de los datos se encuentra entre ellos. Existen pocos valores atipicos por lo que supondremos que los datos son correctos. 

```{r}
summary(pima$diastolic)
hist(pima$diastolic, ylab = "", col = "grey", main = "", breaks = 20)
lines(density(pima$diastolic, na.rm = TRUE))
par(new = TRUE) # Esto indica que lo dibujaremos sobre el gráfico anterior
boxplot(pima$diastolic, horizontal = TRUE, axes = FALSE, lwd = 2, col = rgb(0, 1, 1, alpha = 0.15))
```

La variable triceps tiene pocos datos atipicos pero uno de ellos está especialmente alejado del resto. Este lo interpretamos como un dato atipico y por tanto procedemos a eliminarlo (nos quedamos arbitrariamente con los valores menores que 80).

```{r}
summary(pima$triceps)
hist(pima$triceps, ylab = "", col = "grey", main = "", breaks = 30)
lines(density(pima$triceps, na.rm = TRUE))
par(new = TRUE) # Esto indica que lo dibujaremos sobre el gráfico anterior
boxplot(pima$triceps, horizontal = TRUE, axes = FALSE, lwd = 2, col = rgb(0, 1, 1, alpha = 0.15))
pima = pima[pima$triceps < 80, ] 
```

Observamos como en la variable insulina hay un gran número de valores atipicos, y por ello, procedemos a eliminar esta variable pues los datos no son fiables.

```{r}
summary(pima$insulin)
hist(pima$insulin,  ylab = "", col = "grey", main = "", breaks=30)
lines(density(pima$insulin, na.rm = TRUE))
par(new = TRUE) # Esto indica que lo dibujaremos sobre el gráfico anterior
boxplot(pima$insulin, horizontal = TRUE, axes = FALSE, lwd = 2, col = rgb(0, 1, 1, alpha = 0.15))
pima$insulin <- NULL 
```

La variable bmi tiene una distribución similar a la normal con valores atipicos y al igual que antes hay uno particularmente más alejado del resto, por lo que procedemos a eliminarlo. El resto los mantenemos ya que podrian ser reales aunque se deberia consultar con un experto.

```{r}
summary(pima$bmi)
hist(pima$bmi, ylab = "", col = "grey", main = "", breaks = 20)
lines(density(pima$bmi, na.rm = TRUE))
par(new = TRUE) # Esto indica que lo dibujaremos sobre el gráfico anterior
boxplot(pima$bmi, horizontal = TRUE, axes = FALSE, lwd = 2, col = rgb(0, 1, 1, alpha = 0.15))
pima = pima[pima$bmi < 65, ]
```

La variable diabetes tiene una gran cantidad de valores atipicos, aún así, ante la falta de información en internet sobre qué representa numéricamente esta variable y la imposibilidad de consultar a un experto consideraremos como correctos aquellos valores atipicos que están menos alejados, escogemos "2" como corte en base a lo observado en la gráfica.

```{r}
summary(pima$diabetes)
hist(pima$diabetes, probability = TRUE, ylab = "", col = "grey", main = "", breaks = 20)
lines(density(pima$diabetes)) # lines indica que se va a dibujar una línea sobre el gráfico anterior
par(new = TRUE) # Esto indica que lo dibujaremos sobre el gráfico anterior
boxplot(pima$diabetes, horizontal = TRUE, axes = FALSE, lwd = 2, col = rgb(0, 1, 1, alpha = 0.15))
pima = pima[pima$diabetes < 2, ]
```

Finalmente observamos que la mayoría de personas son jóvenes. Los datos atipicos no los consideramos como erróneos puesto que sí es posible que haya gente mayor (la edad máxima es 81), pese a no ser lo común en el estudio realizado (la media se sitúa en 33 años).

```{r}
summary(pima$age)
hist(pima$age, probability = TRUE, ylab = "", col = "grey", main = "", breaks = 30)
lines(density(pima$age, na.rm = TRUE))
par(new = TRUE) # Esto indica que lo dibujaremos sobre el gráfico anterior
boxplot(pima$age, horizontal = TRUE, axes = FALSE, lwd = 2, col = rgb(0, 1, 1, alpha = 0.15))
```

Por último, estudiamos la relación lineal entre las variables, para esto eliminamos temporalmente la última columna (test) ya que tiene valores no nominales.

```{r}
cov(pima[, 1:7])
```

Estudiamos las correlaciones: hay principalmente 3 pares de variables correlacionadas positivamente. Grosor del pliegue cutáneo del tríceps con bmi, la edad con el número de embarazos, y en menor medida, la presión diastólica con la edad. 

Como podemos ver las correlaciones en general son bajas, también para la variable a predecir (glucose), por lo que nuestro modelo de regresión seguramente no logre buenos resultados a la hora de predecir.

```{r}
par(mfrow = c(1,1))
library('corrplot')
corrplot.mixed(cor(pima[, 1:7], use = "complete.obs"), upper = "circle", lower='number')
```

**c) Analizar en primer lugar un modelo de regresión que incluya todas las variables disponibles (describir el modelo ajustado y sus residuos, contrastar la significatividad individual de los parámetros y la calidad del modelo, verificar las hipótesis del modelo, análisis de datos influyentes y atípicos).**

Creamos un modelo con todas las variables.
```{r}
modelo <- lm(glucose ~ diabetes + pregnant + age + diastolic + triceps + bmi + test, data=pima)
summary(modelo)
```

El modelo de regresión múltiple generado con todas las variables tiene un R^2 ajustado bajo (0.284), es decir, es capaz de explicar un 28.4% de la variabilidad total. La recta de regresion seria:  0.86811*diabetes -0.73475*pregnant + 0.44450*age + 0.24756*diastolic + 0.04428*triceps + 0.25312*bmi + 27.61835*testpositive + 72.06686 

Como podemos observar la variable que más peso tiene es "testpositive" con su coeficiente asociado igual a 27.6183, que, curiosamene, es la unica variable en la que no pudimos estudiar su correlacion al ser una variable categorica. El resto de variables tienen pesos próximos a 0. Las variables diabetes, tríceps y bmi tienen un p-valor muy superior a 0.05, por lo que no podemos rechazar la hipótesis de que no existe relación lineal significativa. Haremos a continuación los contrastes de significatividad individual correspondientes para verificarlo.


Contraste significatividad individual: Observamos que el "0" entra dentro del intervalo de confianza de las variables que acabamos de ver que tenían un p-valor alto, por tanto no superan el contraste de significatividad individual las variables diabetes, triceps y bmi.

```{r}
confint(modelo)
```
En cuanto a los residuos, vamos a mostrarlos gráficamente. Si la relación es lineal, los residuos deben distribuirse aleatoriamente en torno a 0 con una variabilidad constante a lo largo del eje X. Vemos que esto sí se cumple (hay homocedasticidad).

```{r}
par(mfrow=c(1,1))
plot(modelo$fit,modelo$res,xlab="Fitted",ylab="Residuals", main="Residual-Fitted plot")
abline(h=0, col='blue')
```

Vemos que los residuos siguen una distribución normal ya que los puntos se distribuyen aproximadamente sobre la recta del QQ plot y en el histograma podemos ver como parece formar una campana de Gauss.

```{r}
par(mfrow=c(1,2))
qqnorm(modelo$res)
qqline(modelo$res)
hist(modelo$res,10)
```

Comprobamos la independencia mediante el estadístico de Durbin-Watson.

```{r}
library(lmtest)
dwtest(modelo,alternative ="two.sided",iterations = 1000)
```

Como 1.5 <= DW=1.9579 <= 2.5, podemos asumir que los valores son independientes.

Estudiamos los puntos palanca. La línea horizontal marca que los valores por encima de ella son al menos dos veces el efecto medio palanca. Observamos que hay bastantes.

```{r}
x <- model.matrix(modelo)
levevageC <- hat(x)
par(mfrow=c(1,1))
plot(levevageC,ylab="Leverages",main="Index plot of Leverages")
abline(h=2*sum(levevageC)/nrow(pima), lty=2, col="red")
```


Imprimimos los valores con efecto palanca:

```{r}
levevageC [levevageC > 2*sum(levevageC)/nrow(pima)]
```

Usamos la distancia de Cook para hallar puntos influyentes. Serán influyentes aquellos valores que tengan una distancia superior a 4/(n_datos-n_variables-1). Observamos que hay una gran cantidad de puntos influyentes.

```{r}
cookC <- cooks.distance(modelo)
plot(cookC,ylab="Cooks distances")
abline (h =4/(nrow(pima)-7-1), lty = 2, col = "red")
```

Mostramos los puntos influyentes y sus distancias de Cook.

```{r}
cookC[cookC > 4/(nrow(pima)-7-1)]
```

Mediante un influencePlot podemos analizar el efecto palanca, los puntos influyentes y los valores atípicos simultáneamente. 
Observamos una gran cantidad de puntos atipicos (aquellos fuera de las bandas horizontales en -2, 2), con distancias de Cook grandes, representada por el tamaño de la burbuja (punto 538); y con efecto palanca, aquellos que superan la primera línea vertical (representa el doble del efecto palanca) y la segunda (representa el triple del efecto palanca).

```{r}
library(car)
influencePlot(modelo, id.method="identify")
```


##d) Identificar el mejor modelo de regresión lineal y analizarlo.##

Usamos el estadistico Cp de Mallow vemos que el mejor modelo corresponde al 2456, que correspondería en nuestro modelo a las variables 3567.

```{r}
y<- pima$glucose[cooks.distance(modelo) < 0.2]
Y <- y[complete.cases(y)]
x <- pima[,c(1,3,4,5,6,7)]
x <- x[cooks.distance(modelo) < 0.2,]
library(leaps)
a <- leaps(x,Y)
Cpplot(a)
```

Vemos que subconjunto de variables propone el criterio de R^2 ajustado. Este concuerda con lo observado con el estadístico Cp de Mallow proponiendo como mejor modelo el 2456. Vemos aún así que el valor de R^2 es muy bajo (0.13).

```{r}
library(leaps) 
adjr <- leaps(x,Y,method="adjr2")
maxadjr(adjr,8)
```

Buscamos el mejor modelo para las variables explicativas disponibles.

Usando el criterio AIC en combinación con backward/forward/both (no analiza todos los posibles modelos y no considera los p-valores dudosos) nos quedaremos con el modelo que menor AIC tenga pues combina los 2 criterios anteriores.

Como podemos ver los AIC son muy similares, el mejor modelo es el que usa todas las variables menos diabetes y triceps con AIC=4944.8:

```{r}
back <- lm(glucose ~ ., pima)
sm<-step(back, direction = "both")
```

Creamos el modelo con las variables pregnant, diastolic, bmi, age, test:

```{r}
modelo1 <- lm(glucose ~ pregnant + diastolic + bmi + age + test, pima)
summary(modelo1)
```

Observamos que el valor de R^2 ajustado sigue siendo muy bajo e ínfimamente superior al del anterior modelo (ahora es 0.2857 y antes 0.284) aunque no nos sorprende puesto que inicialmente ya vimos como las variables no estaban apenas correlacionadas. La recta de regresión sería:  -0.73147*pregnant + 0.44873*age + 0.24637*diastolic + 0.29623*bmi + 27.72140*testpositive + 72.24965

Los p-valores son todos menores de 0.05 salvo para bmi que es 0.05 prácticamente, para el resto podemos rechazar la hipótesis de que no hay relación lineal significativa. Haremos a continuación los contrastes de significatividad individual correspondientes para verificarlo y ver si la variable bmi es o no significativa.

Vemos que se corrobora lo anterior, todas las variables son significativas salvo bmi pues el "0" pertenece a su intervalo de confianza.

```{r}
confint(modelo1)
```

Analizamos las hipótesis del modelo.

Homocedasticidad: analizamos los residuos y vemos que su varianza es constante gráficamente.

```{r}
plot(modelo1$fit, modelo1$res,xlab="Fitted",ylab="Residuals", main="Residual-Fitted plot")
abline(h=0)
```

Vemos que los residuos siguen una distribución normal ya que los puntos se distribuyen aproximadamente sobre la recta del QQ plot y en el histograma podemos ver como parece formar una campana de Gauss.

```{r}
par(mfrow=c(1,2))
qqnorm(modelo1$res)
qqline(modelo1$res)
hist(modelo1$res,10)
```

Vemos la independencia con el estadístico de Durbin-Watson.

```{r}
dwtest(modelo1,alternative ="two.sided",iterations = 1000)
```

Como 1.5 <= DW = 1.9551 <= 2.5, podemos asumir que los valores son independientes.

Analizamos los puntos palanca, influyentes y atípicos.

```{r}
x <- model.matrix(modelo1)
levevageC <- hat(x)
par(mfrow=c(1,1))
plot(levevageC,ylab="Leverages",main="Index plot of Leverages")
abline(h=2*sum(levevageC)/nrow(pima), lty=2, col="red")
```
La línea horizontal marca que los valores por encima de ella son al menos dos veces el efecto medio palanca. Observamos que hay bastantes.

Imprimimos los valores con efecto palanca:

```{r}
levevageC [levevageC > 2*sum(levevageC)/nrow(pima)]
```

Usamos la distancia de Cook para hallar puntos influyentes. Serán influyentes aquellos valores que tengan una distancia superior a 4/(n_datos-n_variables-1). Observamos que hay una gran cantidad de puntos influyentes.

```{r}
cookC <- cooks.distance(modelo1)
plot(cookC,ylab="Cooks distances")
abline (h =4/(nrow(pima)-5-1), lty = 2, col = "red")
```

Mostramos los puntos influyentes y sus distancias de Cook.

```{r}
cookC[cookC > 4/(nrow(pima)-5-1)]
```

Mediante un influencePlot podemos analizar el efecto palanca, los puntos influyentes y los valores atípicos simultáneamente. 
Observamos una gran cantidad de puntos atipicos (aquellos fuera de las bandas horizontales en -2, 2), con distancias de Cook grandes, representada por el tamaño de la burbuja (punto 538); y con efecto palanca, aquellos que superan la primera línea vertical (representa el doble del efecto palanca) y la segunda (representa el triple del efecto palanca). El punto con mayor efecto palanca sería el 295.

```{r}
library(car)
influencePlot(modelo1, id.method="identify")
```

##e) Analizar el modelo de regresión lineal tras la eliminación de valores influentes y atípicos.##

Hacemos un nuevo modelo eliminando los valores influentes y atípicos:

```{r}
modelo2 <- lm(glucose ~ pregnant + diastolic + bmi + age + test, pima, subset=((cookC <4/(nrow(pima)-5-1))&sort(abs((rstudent(modelo)< qt(0.975,761))))))
summary(modelo2)
```

Vemos que en este modelo la variabilidad total explicada (R^2 ajustado) aumenta considerablemente a un 34.92%. La recta de regresión sería:  -0.67814*pregnant + 0.40397*age + 0.24152*diastolic + 0.31952*bmi + 29.17013*testpositive + 71.98805.

Los p-valores son inferiores todos a 0.05, por lo que se podría rechazar que no existe relación lineal significativa.

Hacemos los contrastes de significatividad individual y observamos como todos lo pasan al no tener ninguno incluido el "0" en su intervalo de confianza.

```{r}
confint(modelo2)
```


Analizamos las hipótesis del modelo.

Homocedasticidad: analizamos los residuos y vemos que su varianza es constante gráficamente.

```{r}
plot(modelo2$fit, modelo2$res,xlab="Fitted",ylab="Residuals", main="Residual-Fitted plot")
abline(h=0)
```

Vemos que los residuos siguen una distribución normal ya que los puntos se distribuyen aproximadamente sobre la recta del QQ plot y en el histograma podemos ver cómo parece formar una campana de Gauss.

```{r}
par(mfrow=c(1,2))
qqnorm(modelo2$res)
qqline(modelo2$res)
hist(modelo2$res,10)
```
Vemos la independencia con el estadístico de Durbin-Watson.

```{r}
dwtest(modelo2,alternative ="two.sided",iterations = 1000)
```

Como 1.5 <= DW = 1.9551 <= 2.5, podemos asumir que los valores son independientes.

Analizamos los puntos palanca, influyentes y atípicos.

```{r}
x <- model.matrix(modelo2)
levevageC <- hat(x)
par(mfrow=c(1,1))
plot(levevageC,ylab="Leverages",main="Index plot of Leverages")
abline(h=2*sum(levevageC)/nrow(pima), lty=2, col="red")
```
La línea horizontal marca que los valores por encima de ella son al menos dos veces el efecto medio palanca. Observamos que hay bastantes.

Imprimimos los valores con efecto palanca:

```{r}
levevageC [levevageC > 2*sum(levevageC)/nrow(pima)]
```

Usamos la distancia de Cook para hallar puntos influyentes. Serán influyentes aquellos valores que tengan una distancia superior a 4/(n_datos-n_variables-1). Observamos que hay una gran cantidad de puntos influyentes.

```{r}
cookC <- cooks.distance(modelo2)
plot(cookC,ylab="Cooks distances")
abline (h =4/(nrow(pima)-5-1), lty = 2, col = "red")
```

Mostramos los puntos influyentes y sus distancias de Cook.

```{r}
cookC[cookC > 4/(nrow(pima)-5-1)]
```

**EJERCICIO 2**

Ahora vamos a realizar el análisis de componentes principales. Para ello importamos las librerías necesarias.

```{r}
library(ISLR)
library(factoextra)
```

El análisis de componentes principales es una técnica de reducción de la dimensionalidad que consiste en transformar nuestras variables en otro conjunto llamado componentes principales que son combinación lineal de las variables iniciales. Las técnicas de reducción de dimensionalidad son muy útiles en el caso de que tengamos un gran número de variables. En nuestro caso, vamos a utilizarlas aunque nuestro número de variables no sea demasiado grande y comparar los modelos que resultan. Cabe destacar que nunca vamos a obtener un modelo mejor que el que tenemos con todas las variables ya que, al quedarnos con sólo algunas componentes, estamos perdiendo siempre información. 

Para utilizar esta técnica, es importante cumplir las siguientes dos condiciones:

  - Las variables son numéricas.
  - Las variables están correlacionadas.

Para cumplir la primera condición, simplemente vamos a retirar de nuestro conjunto de variables la variable test, que es cualitativa. 

```{r}
variables <- pima[,c(1,3,4,5,6,7)]
```

Para la segunda condición, vamos a imprimir los valores de correlación entre nuestras variables. Para ello utilizamos la matriz de correlación de Pearson.

```{r}
Cor<-cor(variables,method="pearson",use="pairwise.complete.obs")
Cor
```

En esta matriz vemos cómo no encontramos casi ningún valor próximo a 1, lo que indicaría una correlación perfecta entre variables. Aún así, vamos a mostrar un gráfico para ver estas correlaciones de manera más visual.

```{r}
library('corrplot')
corrplot(Cor,method="circle")
```

Como podíamos ver anteriormente, el gráfico nos muestra que no hay dos variables que estén muy correlacionadas entre sí. Sí que podemos ver que hay algunas más similares a otras como el bmi y la medida del grosor del pliegue cutáneo en el tríceps, lo cual tiene sentido (una persona con mayor bmi tendrá más sobrepeso por lo que el grosor de su pliegue cutáneo será mayor) o entre el número de embarazos y la edad. 

Una vez vistos estos resultados, vemos como esa segunda condición que necesitamos no se da en nuestros datos o se da de manera muy leve así que nuestro análisis no va a ser bueno. 

Aún así vamos a comprobarlo a continuación.

Esta es la función que nos genera el objeto PCA. Con esta función hacemos todo el análisis y sacamos nuestras componentes principales. Ponemos scale = True para que nuestras variables estén escaladas y no influyan más algunas de ellas por tener un rango de valores mayor.
. 
```{r}
pca_pima<-prcomp(variables,scale=TRUE)
```

Vamos a ver alguna de las características principales del objeto que nos devuelve.

```{r}
pca_pima$rotation
```

Con la columna rotation de nuestro objeto vemos el peso que tiene cada una de nuestras variables en cada una de las componentes principales. Cuanto mayor sea el valor absoluto de un coeficiente, mayor contribución tendrá esa variable en dicha componente. Por ejemplo, vemos como en la componente 4, tenemos la mayoría de la información proveniente de las variables insulin y diabetes.

```{r}
summary(pca_pima)
```

Utilizando la función summary podemos ver la información principal como el porcentaje de la varianza explicada. Este valor indica qué porcentaje de la variabilidad de los datos explica cada una de nuestras variables. La suma de todas ellas resulta 1, es decir, el 100%.  

Podemos observar que no tenemos una PC (componente príncipal) que nos explique la mayor parte de los datos si no que esa ganacia en la varianza explicada se da poco a poco. Esto corrobora que nuestro análisis de componentes principales no va a ser bueno otra vez ya que necesitamos más de 5 componentes para explicar el 90% de la variabilidad. 

```{r}
fviz_eig(pca_pima, addlabels = TRUE, ylim = c(0, 20))
```

El scree plot o gráfico de sedimentos nos muestra de manera gráfica esos datos de variabilidad explicada. Con este gráfico se ve mucho más claro cómo no vemos un gran porcentaje en ninguna de las barras de nuestro histograma y además no hay un cambio significativo entre la variabilidad explicada de las diferentes componentes para poder aplicar el método del codo de la curva y obtener el número de componentes adecuadas.

```{r}
get_eig(pca_pima)
```

Según el método de los autovalores, nos deberíamos quedar con aquellos mayores a 1.

En nuestro caso son las dos primeras componentes. A pesar de esto, antes hemos visto como las tres primeras componentes explicaban apenas un 70% de la variabilidad, una cifra muy pobre.
En nuestro caso vamos a elegir un porcentaje mínimo de varianza explicada de un 90 % con el que nos quedaríamos con 5 componentes. Esta aproximanción la deberíamos hacer con un experto en los datos ya depende mucho de nuestro objetivo y el objetivo del modelo.

Una vez visto el número de PC que queremos, vamos a empezar a analizar cómo se ven los datos representados respecto de ellas.

```{r}
fviz_pca_ind(pca_pima, geom.ind = "point", col.ind = "#FC4E07", axes = c(1, 2), pointsize = 1.5)
```

Con este gráfico podemos ver los datos representados a partir de nuestras dos primeras componentes principales. Vemos como los datos tienen valores un poco más altos en la dimensión uno mientras que, con respecto a la dos, están un poco más aplanados. 
En el siguiente gráfico vamos a ver cómo interviene cada variable en las dos primeras componentes.

```{r}
fviz_pca_var(pca_pima, col.var = "cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)
```

Vemos cómo intervienen nuestras variables iniciales con el gráfico de saturaciones. Las variables que más se alejan del eje vertical son aquellas que intervienen más en la dimensión 1. En este caso son los embarazos, la edad, bmi, presión diastólica y el pliegue cutáneo del triceps. Con respecto a la segunda dimensión, las variables que más se alejan del eje horizontal son las mismas excepto la presión diastólica. Si volvemos a los datos de rotation que hemos impreso antes, los resultados concuenrdas ya que en las dos primeras dimensiones, el peso de las variables es muy similar excepto en la presion diastólica que tiene un valor bastante mayor en la primera.

A continuación, vamos a ver la distribución de los casos de diabéticos y no diabéticos con respecto a las dos primeras componentes. Con este gtráfico podemos comprobar si hay alguna diferencia sustancial entre los pacientes con diabetes y aquellos que no la tienen con respecto a las primeras componentes principales.

```{r}
dataLL<-data.frame(pca_pima$x)
library(car)
scatterplot(dataLL[,2]~ dataLL[,1], xlab= "Componente Principal 1", ylab="Componente principal 2", legend=TRUE,regLine=TRUE, smooth= FALSE, grid=TRUE, groups=pima$test,data= dataLL)
```

Como podemos ver, las dos primeras componentes no diferencian en el valor de test de diabetes.

Con esto damos por terminado nuestro análisis. Hemos podido ver muchas características y gráficos de nuestras PC que nos han indicado que no vamos a mejorar el modelo con esta técnica. Esto ya lo preveíamos antes de empezar al no cumplir una de las dos condiciones iniciales. A pesar de esto, vamos a comprobar igualmente cómo se comporta el modelo con las PC (las nuevas variables) y ver si el análisis que hemos realizado es acertado.

Lo primero vamos a añadir las 5 primeras componentes a nuestro dataframe. Hemos decidido que sean 5 ya que son el mínimo número de componentes con las cuales conseguimos explicar un 90% de la variabilidad.

```{r}
pima$PC1 <- dataLL$PC1
pima$PC2 <- dataLL$PC2
pima$PC3 <- dataLL$PC3
pima$PC4 <- dataLL$PC4
pima$PC5 <- dataLL$PC5
```

Una vez hecho esto imprimimos las correlaciones de nuestro nuevo DataFrame y observamos algunas de sus características. Esto no haría falta ya que son las mismas que en el apartado anterior pero nos sirve para verificar la ortogonalidad de las componentes.

```{r}
cor(pima[,c(1,3,4,5,6,7,9,10,11,12,13)])
```

Nos vamos a centrar en analizar las PC. Como podemos ver, la correlaión entre las diferentes PC es práctiacamente nula. Esto concuerda con la idea de que las PC tienen que ser ortogonales entre sí.

Ahora vamos a analizar el modelo con las PC. 

```{r}
modelo1 <- lm(glucose ~ PC1 + PC2 + PC3+ PC4 +PC5, data=pima)
summary(modelo1)
```

El modelo de regresión resultante es Y = -7.4743 * PC1 -0.8204 * PC2 -2.3835 * PC3 -1.6623 * PC4 -2.3452*PC5
El modelo de regresión múltiple generado con ambas combinaciones de PC tienen un R2 muy bajo (0.1243), es decir, es capaz de explicar un porcentaje muy bajo de la variabilidad observada. El p-valor es muy bajo, por lo que se puede aceptar que el modelo no es fruto del azar. En cuanto al p_valor del test de significatividad individual vemos como solo dos de estos resultan significativos. Esto no tendría mucho sentido en una análisis de componentes principales pero puede ser resultado de lo que hemos dicho antes sobre las variables no correlacionadas.

En cuanto a los residuos, vamos a mostrarlos gráficamente. Si son homocedásticos, los residuos deben de distribuirse aleatoriamente en torno a la recta de regresión con una variabilidad constante a lo largo del eje X.

```{r}
par(mfrow=c(1,1))
plot(modelo1$fit,modelo1$res,xlab="Fitted",ylab="Residuals", main="Residual-Fitted plot")
abline(h=0, col='blue')
```

Vemos que esto sí se cumple ya que no hay una tendencia o patrón en la variabilidad en ninguno de los modelos 

Comprobamos la normalidad de los residuos con el qq-plot y el hostograma. Los residuos son normales si se ajustan a la recta del qq-plot y siguen una distribución similar a una campana en el histograma. Vemos que se cumple también.

```{r}
par(mfrow=c(1,2))
qqnorm(modelo1$res)
qqline(modelo1$res)
hist(modelo1$res,10)
```

Y por último, comprobamos la independencia mediante el estadístico de Durbin-Watson y que la esperanza de los residuos sea 0.

```{r}
library(lmtest)
dwtest(modelo1,alternative ="two.sided",iterations = 1000)
mean(modelo1$res)
```
Podemos comprobar que ambos modelos cumplen todas las condiciones para ser válidos ya que el estadístico de Durbin-Watson está entre 1.5 y 2.5 y la media de los residuos es casi 0.

Aunque el modelo supera todas las condiciones, como hemos dicho antes, este modelo no mejoran para nada los conseguidos en el apartado 1, debido a que no cumplen las condiciones iniciales por lo que era de esperar que no funcionara correctamente. (el R2 es muy bajo)

Como conclusión, determinamos que un modelo de regresión no es el más adecuado para estos datos ya que tenemos una capacidad predictiva baja debido a un bajo R2. Si tenemos en cuenta esto, era obvio que el análisis de componentes principales no nos iba a ayudar a mejorar el modelo, sumado a que hemos visto que los datos no están correlacionados.